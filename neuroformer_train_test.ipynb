{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter\n",
      "CONTRASTIUVEEEEEEE False\n",
      "VISUAL: True\n",
      "PAST_STATE: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path, PurePath\n",
    "path = Path.cwd()\n",
    "parent_path = path.parents[1]\n",
    "sys.path.append(str(PurePath(parent_path, 'neuroformer')))\n",
    "sys.path.append('neuroformer')\n",
    "sys.path.append('.')\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import math\n",
    "\n",
    "from neuroformer.model_neuroformer import Neuroformer, NeuroformerConfig\n",
    "from neuroformer.utils import get_attr\n",
    "from neuroformer.trainer import Trainer, TrainerConfig\n",
    "from neuroformer.utils import (set_seed, update_object, running_jupyter, \n",
    "                                 all_device, load_config, \n",
    "                                 dict_to_object, object_to_dict, recursive_print,\n",
    "                                 create_modalities_dict)\n",
    "from neuroformer.visualize import set_plot_params\n",
    "from neuroformer.data_utils import round_n, Tokenizer, NFDataloader\n",
    "from neuroformer.datasets import load_visnav, load_visnav_2, load_V1AL\n",
    "\n",
    "parent_path = os.path.dirname(os.path.dirname(os.getcwd())) + \"/\"\n",
    "import wandb\n",
    "\n",
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "\n",
    "from neuroformer.default_args import DefaultArgs, parse_args\n",
    "\n",
    "if running_jupyter(): # or __name__ == \"__main__\":\n",
    "    print(\"Running in Jupyter\")\n",
    "    args = DefaultArgs()\n",
    "else:\n",
    "    print(\"Running in terminal\")\n",
    "    args = parse_args()\n",
    "\n",
    "# SET SEED - VERY IMPORTANT\n",
    "set_seed(args.seed)\n",
    "\n",
    "print(f\"CONTRASTIUVEEEEEEE {args.contrastive}\")\n",
    "print(f\"VISUAL: {args.visual}\")\n",
    "print(f\"PAST_STATE: {args.past_state}\")\n",
    "\n",
    "# Use the function\n",
    "if args.config is None:\n",
    "    config_path = \"./configs/Visnav/lateral/mconf_predict_all.yaml\"\n",
    "else:\n",
    "    config_path = args.config\n",
    "config = load_config(config_path)  # replace 'config.yaml' with your file path\n",
    "\n",
    "args.dataset = \"visnav_tigre\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "\n",
    "-- DATA --\n",
    "neuroformer/data/OneCombo3_V1AL/\n",
    "df = response\n",
    "video_stack = stimulus\n",
    "DOWNLOAD DATA URL = https://drive.google.com/drive/folders/1jNvA4f-epdpRmeG9s2E-2Sfo-pwYbjeY?usp=sharing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if args.dataset in [\"lateral\", \"medial\"]:\n",
    "    data, intervals, train_intervals, \\\n",
    "    test_intervals, finetune_intervals, \\\n",
    "    callback = load_visnav(args.dataset, config, \n",
    "                           selection=config.selection if hasattr(config, \"selection\") else None)\n",
    "elif args.dataset == \"V1AL\":\n",
    "    data, intervals, train_intervals, \\\n",
    "    test_intervals, finetune_intervals, \\\n",
    "    callback = load_V1AL(config)\n",
    "elif args.dataset == \"visnav_tigre\":\n",
    "    data, intervals, train_intervals, \\\n",
    "    test_intervals, finetune_intervals, \\\n",
    "    callback = load_visnav_2(\"visnav_tigre\", config, \n",
    "                           selection=config.selection if hasattr(config, \"selection\") else None)\n",
    "\n",
    "\n",
    "\n",
    "spikes = data['spikes']\n",
    "stimulus = data['stimulus']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22315, 100, 290), (3286, 114149))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['vid_sm'].shape, data['spikes'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([780.177, 780.25 , 780.3  , ..., 645.35 , 645.4  , 645.45 ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['spikes', 'speed', 'stimulus', 'phi', 'th', 'depth', 'trialsummary', 'vid_sm'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID vocab size: 3289\n",
      "dt vocab size: 9\n"
     ]
    }
   ],
   "source": [
    "window = config.window.curr\n",
    "window_prev = config.window.prev\n",
    "dt = config.resolution.dt\n",
    "\n",
    "# -------- #\n",
    "\n",
    "spikes_dict = {\n",
    "    \"ID\": data['spikes'],\n",
    "    \"Frames\": data['stimulus'],\n",
    "    \"Interval\": intervals,\n",
    "    \"dt\": config.resolution.dt,\n",
    "    \"id_block_size\": config.block_size.id,\n",
    "    \"prev_id_block_size\": config.block_size.prev_id,\n",
    "    \"frame_block_size\": config.block_size.frame,\n",
    "    \"window\": config.window.curr,\n",
    "    \"window_prev\": config.window.prev,\n",
    "    \"frame_window\": config.window.frame,\n",
    "}\n",
    "\n",
    "\"\"\" \n",
    " - see mconf.yaml \"modalities\" structure:\n",
    "\n",
    "modalities:\n",
    "  behavior:\n",
    "    n_layers: 4\n",
    "    window: 0.05\n",
    "    variables:\n",
    "      speed:\n",
    "        data: speed\n",
    "        dt: 0.05\n",
    "        predict: true\n",
    "        objective: regression\n",
    "      phi:\n",
    "        data: phi\n",
    "        dt: 0.05\n",
    "        predict: true\n",
    "        objective: regression\n",
    "      th:\n",
    "        data: th\n",
    "        dt: 0.05\n",
    "        predict: true\n",
    "        objective: regression\n",
    "\n",
    "\n",
    "Modalities: any additional modalities other than spikes and frames\n",
    "    Behavior: the name of the <modality type>\n",
    "        Variables: the name of the <modality>\n",
    "            Data: the data of the <modality> in shape (n_samples, n_features)\n",
    "            dt: the time resolution of the <modality>, used to index n_samples\n",
    "            Predict: whether to predict this modality or not.\n",
    "                     If you set predict to false, then it will \n",
    "                     not be used as an input in the model,\n",
    "                     but rather to be predicted as an output. \n",
    "            Objective: regression or classification\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "frames = {'feats': stimulus, 'callback': callback, 'window': config.window.frame, 'dt': config.resolution.dt}\n",
    "\n",
    "\n",
    "def configure_token_types(config, modalities):\n",
    "    max_window = max(config.window.curr, config.window.prev)\n",
    "    dt_range = math.ceil(max_window / dt) + 1\n",
    "    n_dt = [round_n(x, dt) for x in np.arange(0, max_window + dt, dt)]\n",
    "\n",
    "    token_types = {\n",
    "        'ID': {'tokens': list(np.arange(0, data['spikes'].shape[0] if isinstance(data['spikes'], np.ndarray) \\\n",
    "                                    else data['spikes'][1].shape[0]))},\n",
    "        'dt': {'tokens': n_dt, 'resolution': dt},\n",
    "        **({\n",
    "            modality: {\n",
    "                'tokens': sorted(list(set(eval(modality)))),\n",
    "                'resolution': details.get('resolution')\n",
    "            }\n",
    "            # if we have to classify the modality, \n",
    "            # then we need to tokenize it\n",
    "            for modality, details in modalities.items() if config.modalities is not None\n",
    "            if details.get('predict', False) and details.get('objective', '') == 'classification'\n",
    "        } if modalities is not None else {})\n",
    "    }\n",
    "    return token_types\n",
    "\n",
    "modalities = create_modalities_dict(data, config.modalities) if get_attr(config, 'modalities', None) else None\n",
    "token_types = configure_token_types(config, modalities)\n",
    "tokenizer = Tokenizer(token_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_layers 4\n",
      "variables {'phi': {'data': array([0.2895439 , 0.2895439 , 0.26832264, ..., 0.26116521, 0.34482745,\n",
      "       0.42458564]), 'dt': 0.05, 'window': 0.05, 'predict': True, 'objective': 'regression'}, 'speed': {'data': array([-2.18456058, -0.74926065, -0.68696415, ...,  0.83853036,\n",
      "        0.95998434,  1.07638287]), 'dt': 0.05, 'window': 0.05, 'predict': True, 'objective': 'regression'}, 'th': {'data': array([1.84228151, 1.84228151, 1.86338685, ..., 0.05941767, 0.04866992,\n",
      "       0.03665915]), 'dt': 0.05, 'window': 0.05, 'predict': True, 'objective': 'regression'}}\n"
     ]
    }
   ],
   "source": [
    "if modalities is not None:\n",
    "    for modality_type, modality in modalities.items():\n",
    "        for variable_type, variable in modality.items():\n",
    "            print(variable_type, variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Interval: 0.1\n",
      "Intervals:  4257\n",
      "Window:  0.05\n",
      "Window Prev:  0.05\n",
      "Population Size:  3289\n",
      "ID Population Size:  3289\n",
      "DT Population Size:  9\n",
      "Using explicitly passed intervals\n",
      "Min Interval: 0.1\n",
      "Intervals:  1080\n",
      "Window:  0.05\n",
      "Window Prev:  0.05\n",
      "Population Size:  3289\n",
      "ID Population Size:  3289\n",
      "DT Population Size:  9\n",
      "Using explicitly passed intervals\n",
      "Min Interval: 0.1\n",
      "Intervals:  40\n",
      "Window:  0.05\n",
      "Window Prev:  0.05\n",
      "Population Size:  3289\n",
      "ID Population Size:  3289\n",
      "DT Population Size:  9\n",
      "Using explicitly passed intervals\n"
     ]
    }
   ],
   "source": [
    "train_dataset = NFDataloader(spikes_dict, tokenizer, config, dataset=args.dataset, \n",
    "                             frames=frames, intervals=train_intervals, modalities=modalities)\n",
    "test_dataset = NFDataloader(spikes_dict, tokenizer, config, dataset=args.dataset, \n",
    "                            frames=frames, intervals=test_intervals, modalities=modalities)\n",
    "finetune_dataset = NFDataloader(spikes_dict, tokenizer, config, dataset=args.dataset, \n",
    "                                frames=frames, intervals=finetune_intervals, modalities=modalities)\n",
    "\n",
    "    \n",
    "# print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')\n",
    "iterable = iter(train_dataset)\n",
    "x, y = next(iterable)\n",
    "# print(x['id'])\n",
    "# print(x['dt'])\n",
    "# recursive_print(x)\n",
    "# recursive_print(y)\n",
    "\n",
    "# # Update the config\n",
    "# config.id_vocab_size = tokenizer.ID_vocab_size\n",
    "# model = Neuroformer(config, tokenizer)\n",
    "\n",
    "# # Create a DataLoader\n",
    "# loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "# iterable = iter(loader)\n",
    "# x, y = next(iterable)\n",
    "# recursive_print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "114149 * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "pbar = tqdm(iterable)\n",
    "for x, y in pbar:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Neuroformer(config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, features, loss = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, features, loss = model(x, y)\n",
    "\n",
    "# Set training parameters\n",
    "MAX_EPOCHS = 250\n",
    "BATCH_SIZE = 32 * 2\n",
    "SHUFFLE = True\n",
    "\n",
    "if config.gru_only:\n",
    "    model_name = \"GRU\"\n",
    "elif config.mlp_only:\n",
    "    model_name = \"MLP\"\n",
    "elif config.gru2_only:\n",
    "    model_name = \"GRU_2.0\"\n",
    "else:\n",
    "    model_name = \"Neuroformer\"\n",
    "\n",
    "CKPT_PATH = f\"./models/NF.15/Visnav_VR_Expt/{args.dataset}/{model_name}/{args.title}/{str(config.layers)}/{args.seed}\"\n",
    "CKPT_PATH = CKPT_PATH.replace(\"namespace\", \"\").replace(\" \", \"_\")\n",
    "\n",
    "if os.path.exists(CKPT_PATH):\n",
    "    counter = 1\n",
    "    print(f\"CKPT_PATH {CKPT_PATH} exists!\")\n",
    "    while os.path.exists(CKPT_PATH + f\"_{counter}\"):\n",
    "        counter += 1\n",
    "\n",
    "if args.resume is not None:\n",
    "    model.load_state_dict(torch.load(args.resume),\n",
    "                           strict=False)\n",
    "\n",
    "if args.sweep_id is not None:\n",
    "    # this is for hyperparameter sweeps\n",
    "    from neuroformer.hparam_sweep import train_sweep\n",
    "    print(f\"-- SWEEP_ID -- {args.sweep_id}\")\n",
    "    wandb.agent(args.sweep_id, function=train_sweep)\n",
    "else:\n",
    "    # Create a TrainerConfig and Trainer\n",
    "    tconf = TrainerConfig(max_epochs=MAX_EPOCHS, batch_size=BATCH_SIZE, learning_rate=1e-4, \n",
    "                          num_workers=16, lr_decay=True, patience=3, warmup_tokens=8e7, \n",
    "                          decay_weights=True, weight_decay=1.0, shuffle=SHUFFLE,\n",
    "                          final_tokens=len(train_dataset)*(config.block_size.id) * (MAX_EPOCHS),\n",
    "                          clip_norm=1.0, grad_norm_clip=1.0,\n",
    "                          show_grads=False,\n",
    "                          ckpt_path=CKPT_PATH, no_pbar=False, \n",
    "                          dist=args.dist, save_every=0, eval_every=5, min_eval_epoch=50,\n",
    "                          use_wandb=False, wandb_project=\"neuroformer\", \n",
    "                          wandb_group=f\"1.5.1_visnav_{args.dataset}\", wandb_name=args.title)\n",
    "\n",
    "    trainer = Trainer(model, train_dataset, test_dataset, tconf, config)\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroformer_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
